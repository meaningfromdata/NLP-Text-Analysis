{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T20:53:00.311921Z",
     "start_time": "2018-01-19T20:52:55.467212Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mostly following this video example of Latent Semantic Analysis and applying it to Darwin Corpus: https://www.youtube.com/watch?v=BJ0MnawUpaU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 'On the Origin of Species (sixth edition)' html: http://www.gutenberg.org/files/2009/2009-h/2009-h.htm\n",
    "\n",
    "# 'The Descent of Man': http://www.gutenberg.org/cache/epub/2300/pg2300.html\n",
    "\n",
    "# 'The Expression of the Emotions in Man and Animals': http://www.gutenberg.org/files/1227/1227-h/1227-h.htm\n",
    "\n",
    "\n",
    "# Importing requests, BeautifulSoup and nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T20:55:34.236075Z",
     "start_time": "2018-01-19T20:55:31.999048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the html for all three books from Project Gutenberg website\n",
    "\n",
    "origin = requests.get('http://www.gutenberg.org/files/2009/2009-h/2009-h.htm')\n",
    "descent = requests.get('http://www.gutenberg.org/cache/epub/2300/pg2300.html')\n",
    "emotions = requests.get('http://www.gutenberg.org/files/1227/1227-h/1227-h.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T21:03:17.214165Z",
     "start_time": "2018-01-19T21:03:16.986971Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the correct text encoding of the HTML page for origin \n",
    "origin.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "origin_html = origin.text\n",
    "\n",
    "# Creating a BeautifulSoup object from the origin HTML\n",
    "origin_soup = BeautifulSoup(origin_html, 'html.parser')\n",
    "\n",
    "# Getting the text out of the origin soup\n",
    "origin_text = origin_soup.get_text()\n",
    "\n",
    "\n",
    "\n",
    "# TOKENIZATION WITH NLTK COMMENTED OUT BECAUSE IT IS NOT NECESSARY IF USING sci-kit learn \n",
    "\n",
    "# Creating a tokenizer\n",
    "# tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenizing the origin text\n",
    "# origin_tokens = tokenizer.tokenize(origin_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T15:55:42.578683Z",
     "start_time": "2018-01-20T15:55:42.567676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str, str, bs4.BeautifulSoup, str]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(origin.encoding), type(origin_html), type(origin_soup), type(origin_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T21:03:52.237018Z",
     "start_time": "2018-01-19T21:03:51.707612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the correct text encoding of the HTML page for descent\n",
    "descent.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "descent_html = descent.text\n",
    "\n",
    "# Creating a BeautifulSoup object from the descent HTML\n",
    "descent_soup = BeautifulSoup(descent_html, 'html.parser')\n",
    "\n",
    "# Getting the text out of the descent soup\n",
    "descent_text = descent_soup.get_text()\n",
    "\n",
    "# Tokenizing the descent text\n",
    "# descent_tokens = tokenizer.tokenize(descent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T21:04:22.755078Z",
     "start_time": "2018-01-19T21:04:22.440824Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the correct text encoding of the HTML page for emotions\n",
    "emotions.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "emotions_html = emotions.text\n",
    "\n",
    "# Creating a BeautifulSoup object from the emotions HTML\n",
    "emotions_soup = BeautifulSoup(emotions_html, 'html.parser')\n",
    "\n",
    "# Getting the text out of the emotions soup\n",
    "emotions_text = emotions_soup.get_text()\n",
    "\n",
    "# Tokenizing the emotions text\n",
    "# emotions_tokens = tokenizer.tokenize(emotions_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:34:52.724693Z",
     "start_time": "2018-01-19T22:34:52.699673Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:41:11.607130Z",
     "start_time": "2018-01-19T22:41:11.603128Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a single list containing all three texts (untokenized) books that will be treated as a corpus for tf-idf\n",
    "\n",
    "darwin_corpus = [origin_text, descent_text, emotions_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:41:18.110035Z",
     "start_time": "2018-01-19T22:41:14.196309Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the TfidfVectorizer allowing for ngrams of 1, 2 and 3 words in length to be used as features\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "# fit the TfidfVectorizer to darwin_corpus and then generate document-term sparse matrix ('X') with Tf-idf scores\n",
    "\n",
    "X = vectorizer.fit_transform(darwin_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T19:46:42.887332Z",
     "start_time": "2018-01-20T19:46:42.777475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 552738)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is 3 x 552738 matrix (3 books --the rows-- in corpus and 552738 tokens/features/words --the columns-- extracted from corpus)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:41:51.471642Z",
     "start_time": "2018-01-19T22:41:51.449627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x552738 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 189860 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:50:45.038817Z",
     "start_time": "2018-01-19T22:50:45.023306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 361095)\t0.0191015987011\n",
      "  (0, 466036)\t0.460826068665\n",
      "  (0, 456387)\t0.00238769983764\n",
      "  (0, 156569)\t0.00429785970776\n",
      "  (0, 93249)\t0.00358154975646\n",
      "  (0, 126948)\t0.00191015987011\n",
      "  (0, 69202)\t0.0157588189284\n",
      "  (0, 308544)\t0.00716309951293\n",
      "  (0, 53776)\t0.000614919655066\n",
      "  (0, 183970)\t0.000404272724813\n",
      "  (0, 496122)\t0.00214892985388\n",
      "  (0, 27345)\t0.00215221879273\n",
      "  (0, 269083)\t0.000716309951293\n",
      "  (0, 252614)\t0.000614919655066\n",
      "  (0, 6223)\t0.000307459827533\n",
      "  (0, 506312)\t0.00143261990259\n",
      "  (0, 7310)\t0.000614919655066\n",
      "  (0, 70965)\t0.00310400978894\n",
      "  (0, 228837)\t0.000307459827533\n",
      "  (0, 228840)\t0.000307459827533\n",
      "  (0, 228843)\t0.000307459827533\n",
      "  (0, 228846)\t0.000307459827533\n",
      "  (0, 228849)\t0.000307459827533\n",
      "  (0, 228852)\t0.000307459827533\n",
      "  (0, 87206)\t0.000922379482599\n",
      "  :\t:\n",
      "  (0, 533792)\t0.000238769983764\n",
      "  (0, 456063)\t0.000238769983764\n",
      "  (0, 296133)\t0.000238769983764\n",
      "  (0, 377157)\t0.000238769983764\n",
      "  (0, 433768)\t0.000404272724813\n",
      "  (0, 182509)\t0.000404272724813\n",
      "  (0, 228752)\t0.000238769983764\n",
      "  (0, 359372)\t0.000238769983764\n",
      "  (0, 533791)\t0.000238769983764\n",
      "  (0, 456061)\t0.000238769983764\n",
      "  (0, 251174)\t0.000238769983764\n",
      "  (0, 228768)\t0.000238769983764\n",
      "  (0, 504864)\t0.000238769983764\n",
      "  (0, 251303)\t0.000238769983764\n",
      "  (0, 296691)\t0.000238769983764\n",
      "  (0, 44904)\t0.000238769983764\n",
      "  (0, 208232)\t0.000238769983764\n",
      "  (0, 235994)\t0.000238769983764\n",
      "  (0, 395843)\t0.000238769983764\n",
      "  (0, 341187)\t0.000238769983764\n",
      "  (0, 156103)\t0.000238769983764\n",
      "  (0, 483529)\t0.000238769983764\n",
      "  (0, 160300)\t0.000238769983764\n",
      "  (0, 341711)\t0.000238769983764\n",
      "  (0, 235004)\t0.000238769983764\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T19:59:37.179369Z",
     "start_time": "2018-01-20T19:59:36.095598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '000 000',\n",
       " '000 000 soldiers',\n",
       " '000 15',\n",
       " '000 15 000',\n",
       " '000 according',\n",
       " '000 according loose',\n",
       " '000 feet',\n",
       " '000 feet deeply',\n",
       " '000 feet informs',\n",
       " '000 feet professor',\n",
       " '000 feet solid',\n",
       " '000 feet valleys',\n",
       " '000 feet yet',\n",
       " '000 number',\n",
       " '000 number soon',\n",
       " '000 padding',\n",
       " '000 padding 5em',\n",
       " '000 particularly',\n",
       " '000 particularly important',\n",
       " '000 pieces',\n",
       " '000 pieces shell',\n",
       " '000 pigeons',\n",
       " '000 pigeons taken',\n",
       " '000 soldiers',\n",
       " '000 soldiers served',\n",
       " '000 specimens',\n",
       " '000 specimens apus',\n",
       " '000 years',\n",
       " '000 years ago',\n",
       " '000 years respect',\n",
       " '019',\n",
       " '019 81',\n",
       " '019 81 1860',\n",
       " '050',\n",
       " '050 1832',\n",
       " '050 1832 several',\n",
       " '071',\n",
       " '071 males',\n",
       " '071 males 25',\n",
       " '08',\n",
       " '08 100',\n",
       " '08 100 time',\n",
       " '084',\n",
       " '084 18',\n",
       " '084 18 1866',\n",
       " '09',\n",
       " '09 inch',\n",
       " '09 inch therefore',\n",
       " '10',\n",
       " '10 000',\n",
       " '10 000 15',\n",
       " '10 100',\n",
       " '10 100 plants',\n",
       " '10 12',\n",
       " '10 12 35',\n",
       " '10 1857',\n",
       " '10 1857 regard',\n",
       " '10 1868',\n",
       " '10 1868 724',\n",
       " '10 1869',\n",
       " '10 1869 lately',\n",
       " '10 773',\n",
       " '10 773 females',\n",
       " '10 87',\n",
       " '10 87 state',\n",
       " '10 almost',\n",
       " '10 almost actions',\n",
       " '10 bridgewater',\n",
       " '10 bridgewater treatise',\n",
       " '10 cervulus',\n",
       " '10 cervulus dr',\n",
       " '10 disgust',\n",
       " '10 disgust shown',\n",
       " '10 doubleday',\n",
       " '10 doubleday annals',\n",
       " '10 dr',\n",
       " '10 dr savage',\n",
       " '10 figs',\n",
       " '10 figs 27',\n",
       " '10 font',\n",
       " '10 font family',\n",
       " '10 given',\n",
       " '10 given evidence',\n",
       " '10 good',\n",
       " '10 good opportunities',\n",
       " '10 gorilla',\n",
       " '10 gorilla savage',\n",
       " '10 gould',\n",
       " '10 gould handbook',\n",
       " '10 hereditary',\n",
       " '10 hereditary genius',\n",
       " '10 inhabit',\n",
       " '10 inhabit different',\n",
       " '10 journal',\n",
       " '10 journal travel',\n",
       " '10 macgillivray',\n",
       " '10 macgillivray history',\n",
       " '10 male',\n",
       " '10 male left',\n",
       " '10 margin',\n",
       " '10 margin bottom',\n",
       " '10 margin right',\n",
       " '10 mr',\n",
       " '10 mr belt',\n",
       " '10 mr blyth',\n",
       " '10 mr wallace',\n",
       " '10 mynote',\n",
       " '10 mynote background',\n",
       " '10 often',\n",
       " '10 often observed',\n",
       " '10 origin',\n",
       " '10 origin species',\n",
       " '10 project',\n",
       " '10 project gutenberg',\n",
       " '10 reflex',\n",
       " '10 reflex movements',\n",
       " '10 rengger',\n",
       " '10 rengger naturgeschichte',\n",
       " '10 return',\n",
       " '10 return essays',\n",
       " '10 see',\n",
       " '10 see interesting',\n",
       " '10 see paper',\n",
       " '10 see yarrell',\n",
       " '10 sound',\n",
       " '10 sound producing',\n",
       " '10 taphroderes',\n",
       " '10 taphroderes distortus',\n",
       " '10 tour',\n",
       " '10 tour sutherlandshire',\n",
       " '10 valuable',\n",
       " '10 valuable essay',\n",
       " '10 woodcut',\n",
       " '10 woodcut kallima',\n",
       " '100',\n",
       " '100 104',\n",
       " '100 104 temporary',\n",
       " '100 111',\n",
       " '100 111 194',\n",
       " '100 143',\n",
       " '100 143 respect',\n",
       " '100 149',\n",
       " '100 149 apparently',\n",
       " '100 49',\n",
       " '100 49 norway',\n",
       " '100 50',\n",
       " '100 50 regard',\n",
       " '100 another',\n",
       " '100 another striking',\n",
       " '100 catching',\n",
       " '100 catching animals',\n",
       " '100 census',\n",
       " '100 census 1872',\n",
       " '100 christian',\n",
       " '100 christian births',\n",
       " '100 colonel',\n",
       " '100 colonel marshall',\n",
       " '100 curious',\n",
       " '100 curious observe',\n",
       " '100 desmarest',\n",
       " '100 desmarest mammalogie',\n",
       " '100 developed',\n",
       " '100 developed aid',\n",
       " '100 enquiries',\n",
       " '100 enquiries made',\n",
       " '100 etc',\n",
       " '100 etc also',\n",
       " '100 even',\n",
       " '100 even small',\n",
       " '100 female',\n",
       " '100 female births',\n",
       " '100 female children',\n",
       " '100 females',\n",
       " '100 females 1857',\n",
       " '100 females average',\n",
       " '100 females census',\n",
       " '100 females greatest',\n",
       " '100 females hand',\n",
       " '100 females important',\n",
       " '100 females numbers',\n",
       " '100 females respect',\n",
       " '100 females rev',\n",
       " '100 females sandwich',\n",
       " '100 females singular',\n",
       " '100 females tabulated',\n",
       " '100 females take',\n",
       " '100 females todas',\n",
       " '100 females whereas',\n",
       " '100 females year',\n",
       " '100 genera',\n",
       " '100 genera indigenous',\n",
       " '100 geographical',\n",
       " '100 geographical miles',\n",
       " '100 girls',\n",
       " '100 girls proportion',\n",
       " '100 greyhounds',\n",
       " '100 greyhounds inequality',\n",
       " '100 heads',\n",
       " '100 heads red',\n",
       " '100 however',\n",
       " '100 however degree',\n",
       " '100 illustrations',\n",
       " '100 illustrations pritchett',\n",
       " '100 kinds',\n",
       " '100 kinds plants',\n",
       " '100 land',\n",
       " '100 land shells',\n",
       " '100 looking',\n",
       " '100 looking separate',\n",
       " '100 males',\n",
       " '100 males seventeen',\n",
       " '100 mares',\n",
       " '100 mares born',\n",
       " '100 may',\n",
       " '100 may added',\n",
       " '100 must',\n",
       " '100 must borne',\n",
       " '100 period',\n",
       " '100 period occurred',\n",
       " '100 plants',\n",
       " '100 plants belonging',\n",
       " '100 plants flora',\n",
       " '100 pounds',\n",
       " '100 pounds male',\n",
       " '100 probably',\n",
       " '100 probably nearly',\n",
       " '100 ratio',\n",
       " '100 ratio throughout',\n",
       " '100 regard',\n",
       " '100 regard domestic',\n",
       " '100 scarcely',\n",
       " '100 scarcely possible',\n",
       " '100 see',\n",
       " '100 see also',\n",
       " '100 sheep',\n",
       " '100 sheep age',\n",
       " '100 similarly',\n",
       " '100 similarly coloured',\n",
       " '100 squirrels',\n",
       " '100 squirrels audubon',\n",
       " '100 still',\n",
       " '100 still born',\n",
       " '100 taking',\n",
       " '100 taking still',\n",
       " '100 time',\n",
       " '100 time number',\n",
       " '100 whilst',\n",
       " '100 whilst wales',\n",
       " '1000',\n",
       " '1000 measurements',\n",
       " '1000 measurements reduced',\n",
       " '1000 unmarried',\n",
       " '1000 unmarried men',\n",
       " '1001',\n",
       " '1001 chickens',\n",
       " '1001 chickens highly',\n",
       " '1001 emotions',\n",
       " '1001 emotions closely',\n",
       " '1001 return',\n",
       " '1001 return see',\n",
       " '1002',\n",
       " '1002 even',\n",
       " '1002 even said',\n",
       " '1002 return',\n",
       " '1002 return rengger',\n",
       " '1003',\n",
       " '1003 monkeys',\n",
       " '1003 monkeys also',\n",
       " '1003 return',\n",
       " '1003 return sir',\n",
       " '1004',\n",
       " '1004 men',\n",
       " '1004 men heart',\n",
       " '1004 return',\n",
       " '1004 return moreau',\n",
       " '1005',\n",
       " '1005 return',\n",
       " '1005 return sir',\n",
       " '1005 tennyson',\n",
       " '1005 tennyson writes',\n",
       " '1006',\n",
       " '1006 excited',\n",
       " '1006 excited brain',\n",
       " '1006 return',\n",
       " '1006 return mr',\n",
       " '1007',\n",
       " '1007 rendered',\n",
       " '1007 rendered loud',\n",
       " '1007 return',\n",
       " '1007 return sir',\n",
       " '1008',\n",
       " '1008 return',\n",
       " '1008 return de',\n",
       " '1008 shakspeare',\n",
       " '1008 shakspeare sums',\n",
       " '1009',\n",
       " '1009 appearance',\n",
       " '1009 appearance teeth',\n",
       " '1009 return',\n",
       " '1009 return sir',\n",
       " '101',\n",
       " '101 143',\n",
       " '101 143 dorsal',\n",
       " '101 arrived',\n",
       " '101 arrived however',\n",
       " '101 assuredly',\n",
       " '101 assuredly error',\n",
       " '101 difficult',\n",
       " '101 difficult approach',\n",
       " '101 remains',\n",
       " '101 remains dispute',\n",
       " '101 return',\n",
       " '101 return art',\n",
       " '101 return mr',\n",
       " '101 violent',\n",
       " '101 violent laughter',\n",
       " '1010',\n",
       " '1010 return',\n",
       " '1010 return oliver',\n",
       " '1010 speaking',\n",
       " '1010 speaking atrocious',\n",
       " '1011',\n",
       " '1011 retraction',\n",
       " '1011 retraction lips',\n",
       " '1011 return',\n",
       " '1011 return spectator',\n",
       " '1012',\n",
       " '1012 question',\n",
       " '1012 question must',\n",
       " '1012 return',\n",
       " '1012 return body',\n",
       " '1013',\n",
       " '1013 figures',\n",
       " '1013 figures plate',\n",
       " '1013 return',\n",
       " '1013 return le',\n",
       " '1014',\n",
       " '1014 mr',\n",
       " '1014 mr rejlander',\n",
       " '1014 return',\n",
       " '1014 return transact',\n",
       " '1015',\n",
       " '1015 actor',\n",
       " '1015 actor cooke',\n",
       " '1015 return',\n",
       " '1015 return anatomy',\n",
       " '1016',\n",
       " '1016 return',\n",
       " '1016 return hensleigh',\n",
       " '1016 suspect',\n",
       " '1016 suspect see',\n",
       " '1017',\n",
       " '1017 may',\n",
       " '1017 may suspect',\n",
       " '1017 return',\n",
       " '1017 return descent',\n",
       " '102',\n",
       " '102 100',\n",
       " '102 100 whilst',\n",
       " '102 103',\n",
       " '102 103 half',\n",
       " '102 conducting',\n",
       " '102 conducting power',\n",
       " '102 duchenne',\n",
       " '102 duchenne mecanisme',\n",
       " '102 family',\n",
       " '102 family tits',\n",
       " '102 far',\n",
       " '102 far cerebral',\n",
       " '102 near',\n",
       " '102 near bogota',\n",
       " '102 return',\n",
       " '102 return muller',\n",
       " '102 since',\n",
       " '102 since partly',\n",
       " '102 species',\n",
       " '102 species however',\n",
       " '102 subjects',\n",
       " '102 subjects varieties',\n",
       " '103',\n",
       " '103 118',\n",
       " '103 118 1403',\n",
       " '103 caterpillars',\n",
       " '103 caterpillars fed',\n",
       " '103 figures',\n",
       " '103 figures given',\n",
       " '103 half',\n",
       " '103 half wild',\n",
       " '103 mr',\n",
       " '103 mr harrison',\n",
       " '103 philosophical',\n",
       " '103 philosophical transactions',\n",
       " '103 rather',\n",
       " '103 rather betray',\n",
       " '103 return',\n",
       " '103 return remark',\n",
       " '103 smith',\n",
       " '103 smith record',\n",
       " '103 tears',\n",
       " '103 tears irritate',\n",
       " '104',\n",
       " '104 100',\n",
       " '104 100 50',\n",
       " '104 100 looking',\n",
       " '104 100 ratio',\n",
       " '104 always',\n",
       " '104 always perceived',\n",
       " '104 days',\n",
       " '104 days one',\n",
       " '104 intellectual',\n",
       " '104 intellectual faculties',\n",
       " '104 males',\n",
       " '104 males 100',\n",
       " '104 many',\n",
       " '104 many cases',\n",
       " '104 order',\n",
       " '104 order hymenoptera',\n",
       " '104 respect',\n",
       " '104 respect cattle',\n",
       " '104 return',\n",
       " '104 return see',\n",
       " '104 russia',\n",
       " '104 russia 108',\n",
       " '104 sir',\n",
       " '104 sir lubbock',\n",
       " '104 stone',\n",
       " '104 stone arrow',\n",
       " '104 temporary',\n",
       " '104 temporary hook',\n",
       " '1040',\n",
       " '1040 corpses',\n",
       " '1040 corpses often',\n",
       " '105',\n",
       " '105 1215',\n",
       " '105 1215 return',\n",
       " '105 1865',\n",
       " '105 1865 104',\n",
       " '105 analogous',\n",
       " '105 analogous statements',\n",
       " '105 fairly',\n",
       " '105 fairly developed',\n",
       " '105 good',\n",
       " '105 good authority',\n",
       " '105 important',\n",
       " '105 important purpose',\n",
       " '105 insensibly',\n",
       " '105 insensibly crown',\n",
       " '105 remarkable',\n",
       " '105 remarkable throughout',\n",
       " '105 respect',\n",
       " '105 respect colour',\n",
       " '105 return',\n",
       " '105 return senses',\n",
       " '105 skulls',\n",
       " '105 skulls many',\n",
       " '106',\n",
       " '106 100',\n",
       " '106 100 period',\n",
       " '106 100 taking',\n",
       " '106 1204',\n",
       " '106 1204 return',\n",
       " '106 animals',\n",
       " '106 animals appear',\n",
       " '106 continued',\n",
       " '106 continued use',\n",
       " '106 males',\n",
       " '106 males 100',\n",
       " '106 muleteers',\n",
       " '106 muleteers america',\n",
       " '106 return',\n",
       " '106 return gratiolet',\n",
       " '106 rudiment',\n",
       " '106 rudiment apparently',\n",
       " '106 see',\n",
       " '106 see also',\n",
       " '1068',\n",
       " '1068 nothing',\n",
       " '1068 nothing common',\n",
       " '107',\n",
       " '107 126',\n",
       " '107 126 131',\n",
       " '107 1867',\n",
       " '107 1867 92',\n",
       " '107 american',\n",
       " '107 american monkey',\n",
       " '107 ateles',\n",
       " '107 ateles desmarest',\n",
       " '107 dr',\n",
       " '107 dr cullen',\n",
       " '107 eastward',\n",
       " '107 eastward india',\n",
       " '107 fully',\n",
       " '107 fully discussed',\n",
       " '107 one',\n",
       " '107 one stag',\n",
       " '107 one year',\n",
       " '107 person',\n",
       " '107 person trying',\n",
       " '107 return',\n",
       " '107 return mecanisme',\n",
       " '107 vogt',\n",
       " '107 vogt remarks',\n",
       " '108',\n",
       " '108 118',\n",
       " '108 118 additional',\n",
       " '108 579',\n",
       " '108 579 47',\n",
       " '108 another',\n",
       " '108 another curious',\n",
       " '108 gould',\n",
       " '108 gould handbook',\n",
       " '108 jews',\n",
       " '108 jews livonia',\n",
       " '108 little',\n",
       " '108 little known',\n",
       " '108 mr',\n",
       " '108 mr blyth',\n",
       " '108 mr brown',\n",
       " '108 philadelphia',\n",
       " '108 philadelphia united',\n",
       " '108 return',\n",
       " '108 return variation',\n",
       " '1085',\n",
       " '1085 influence',\n",
       " '1085 influence attention',\n",
       " '109',\n",
       " '109 111',\n",
       " '109 111 308',\n",
       " '109 149',\n",
       " '109 149 deserves',\n",
       " '109 49',\n",
       " '109 49 100',\n",
       " '109 also',\n",
       " '109 also owen',\n",
       " '109 hatred',\n",
       " '109 hatred indecency',\n",
       " '109 may',\n",
       " '109 may remind',\n",
       " '109 mr',\n",
       " '109 mr allen',\n",
       " '109 reflex',\n",
       " '109 reflex actions',\n",
       " '109 return',\n",
       " '109 return prof',\n",
       " '109 whilst',\n",
       " '109 whilst sitting',\n",
       " '10th',\n",
       " '10th first',\n",
       " '10th first shewed',\n",
       " '11',\n",
       " '11 1120',\n",
       " '11 1120 return',\n",
       " '11 1868',\n",
       " '11 1868 810',\n",
       " '11 annals',\n",
       " '11 annals magazine',\n",
       " '11 annually',\n",
       " '11 annually died',\n",
       " '11 audubon',\n",
       " '11 audubon ornithological',\n",
       " '11 authors',\n",
       " '11 authors written',\n",
       " '11 bates',\n",
       " '11 bates journal',\n",
       " '11 case',\n",
       " '11 case females',\n",
       " '11 consists',\n",
       " '11 consists described',\n",
       " '11 critic',\n",
       " '11 critic without',\n",
       " '11 dearer',\n",
       " '11 dearer females',\n",
       " '11 examination',\n",
       " '11 examination prof',\n",
       " '11 extreme',\n",
       " '11 extreme fear',\n",
       " '11 female',\n",
       " '11 female whole',\n",
       " '11 gryllus',\n",
       " '11 gryllus campestris',\n",
       " '11 hen',\n",
       " '11 hen driving',\n",
       " '11 hist',\n",
       " '11 hist nat',\n",
       " '11 indebted',\n",
       " '11 indebted dr',\n",
       " '11 introduction',\n",
       " '11 introduction classification',\n",
       " '11 mares',\n",
       " '11 mares diversis',\n",
       " '11 mr',\n",
       " '11 mr bates',\n",
       " '11 mr hewitt',\n",
       " '11 mr wallace',\n",
       " '11 natural',\n",
       " '11 natural philosophers',\n",
       " '11 owen',\n",
       " '11 owen anatomy',\n",
       " '11 pallas',\n",
       " '11 pallas spicilegia',\n",
       " '11 pike',\n",
       " '11 pike separated',\n",
       " '11 prehistoric',\n",
       " '11 prehistoric times',\n",
       " '11 prof',\n",
       " '11 prof fick',\n",
       " '11 quoted',\n",
       " '11 quoted farmer',\n",
       " '11 remarks',\n",
       " '11 remarks negative',\n",
       " '11 respects',\n",
       " '11 respects question',\n",
       " '11 return',\n",
       " '11 return since',\n",
       " '11 see',\n",
       " '11 see subject',\n",
       " '11 tylor',\n",
       " '11 tylor ibid',\n",
       " '11 wanderings',\n",
       " '11 wanderings new',\n",
       " '110',\n",
       " '110 100',\n",
       " '110 100 49',\n",
       " '110 100 enquiries',\n",
       " '110 100 however',\n",
       " '110 100 probably',\n",
       " '110 131',\n",
       " '110 131 expose',\n",
       " '110 1342',\n",
       " '110 1342 return',\n",
       " '110 1346',\n",
       " '110 1346 return',\n",
       " '110 1402',\n",
       " '110 1402 return',\n",
       " '110 admitted',\n",
       " '110 admitted perceive',\n",
       " '110 days',\n",
       " '110 days third',\n",
       " '110 desor',\n",
       " '110 desor 15',\n",
       " '110 males',\n",
       " '110 males 100',\n",
       " '110 mind',\n",
       " '110 mind brain',\n",
       " '110 mr',\n",
       " '110 mr jenner',\n",
       " '110 return',\n",
       " '110 return dr',\n",
       " '110 see',\n",
       " '110 see difference',\n",
       " '110 western',\n",
       " '110 western islands',\n",
       " '1101',\n",
       " '1101 descriptions',\n",
       " '1101 descriptions trustworthy',\n",
       " '1101 return',\n",
       " '1101 return de',\n",
       " '1102',\n",
       " '1102 insists',\n",
       " '1102 insists turning',\n",
       " '1102 return',\n",
       " '1102 return physionomie',\n",
       " '1103',\n",
       " '1103 commonly',\n",
       " '1103 commonly accompanied',\n",
       " '1103 return',\n",
       " '1103 return dr',\n",
       " '1104',\n",
       " '1104 new',\n",
       " '1104 new zealanders',\n",
       " '1104 protrude',\n",
       " '1104 protrude raise',\n",
       " '1104 return',\n",
       " '1104 return mimik',\n",
       " '1105',\n",
       " '1105 nearly',\n",
       " '1105 nearly manner',\n",
       " '1105 return',\n",
       " '1105 return scorn',\n",
       " '1106',\n",
       " '1106 intelligible',\n",
       " '1106 intelligible generally',\n",
       " '1106 return',\n",
       " '1106 return early',\n",
       " '1107',\n",
       " '1107 extreme',\n",
       " '1107 extreme disgust',\n",
       " '1107 return',\n",
       " '1107 return see',\n",
       " '1108',\n",
       " '1108 remarkable',\n",
       " '1108 remarkable readily',\n",
       " '1108 return',\n",
       " '1108 return duchenne',\n",
       " '1109',\n",
       " '1109 mr',\n",
       " '1109 mr scott',\n",
       " '1109 return',\n",
       " '1109 return quoted',\n",
       " '111',\n",
       " '111 113',\n",
       " '111 113 163',\n",
       " '111 194',\n",
       " '111 194 196',\n",
       " '111 308',\n",
       " '111 308 return',\n",
       " '111 action',\n",
       " '111 action musculus',\n",
       " '111 certain',\n",
       " '111 certain groups',\n",
       " '111 conscious',\n",
       " '111 conscious wish',\n",
       " '111 discussion',\n",
       " '111 discussion laughter',\n",
       " '111 given',\n",
       " '111 given journal',\n",
       " '111 insists',\n",
       " '111 insists beautiful',\n",
       " '111 respect',\n",
       " '111 respect swan',\n",
       " '111 return',\n",
       " '111 return see',\n",
       " '111 souls',\n",
       " '111 souls 1835',\n",
       " '1110',\n",
       " '1110 never',\n",
       " '1110 never saw',\n",
       " '1110 return',\n",
       " '1110 return quotations',\n",
       " '1111',\n",
       " '1111 return',\n",
       " '1111 return stated',\n",
       " '1111 seen',\n",
       " '1111 seen scorn',\n",
       " '1112',\n",
       " '1112 desire',\n",
       " '1112 desire see',\n",
       " '1112 return',\n",
       " '1112 return principles',\n",
       " '1113',\n",
       " '1113 arrogant',\n",
       " '1113 arrogant man',\n",
       " '1113 return',\n",
       " '1113 return gratiolet',\n",
       " '1114',\n",
       " '1114 life',\n",
       " '1114 life like',\n",
       " '1114 return',\n",
       " '1114 return anatomy',\n",
       " '1115',\n",
       " '1115 indian',\n",
       " '1115 indian texas',\n",
       " '1115 return',\n",
       " '1115 return journey',\n",
       " '1116',\n",
       " '1116 describing',\n",
       " '1116 describing young',\n",
       " '1116 return',\n",
       " '1116 return mrs',\n",
       " '1117',\n",
       " '1117 deserves',\n",
       " '1117 deserves notice',\n",
       " '1117 owen',\n",
       " '1117 owen anatomy',\n",
       " '1117 return',\n",
       " '1117 return essai',\n",
       " '1118',\n",
       " '1118 return',\n",
       " '1118 return origin',\n",
       " '1118 voice',\n",
       " '1118 voice exerted',\n",
       " '1119',\n",
       " '1119 imagined',\n",
       " '1119 imagined gestures',\n",
       " '1119 return',\n",
       " '1119 return vocal',\n",
       " '112',\n",
       " '112 100',\n",
       " '112 100 census',\n",
       " '112 114',\n",
       " '112 114 several',\n",
       " '112 attention',\n",
       " '112 attention paid',\n",
       " '112 describing',\n",
       " '112 describing beauty',\n",
       " '112 difference',\n",
       " '112 difference 139',\n",
       " '112 males',\n",
       " '112 males 84',\n",
       " '112 marcus',\n",
       " '112 marcus aurelius',\n",
       " '112 mr',\n",
       " '112 mr buckler',\n",
       " '112 return',\n",
       " '112 return chapters',\n",
       " '1120',\n",
       " '1120 answering',\n",
       " '1120 answering asked',\n",
       " '1120 return',\n",
       " '1120 return memoire',\n",
       " '1121',\n",
       " '1121 nevertheless',\n",
       " '1121 nevertheless look',\n",
       " '1121 return',\n",
       " '1121 return quoted',\n",
       " '1122',\n",
       " '1122 return',\n",
       " '1122 return mr',\n",
       " '1122 throwing',\n",
       " '1122 throwing back',\n",
       " '1123',\n",
       " '1123 abyssinians',\n",
       " '1123 abyssinians informed',\n",
       " '1123 return',\n",
       " '1123 return lieber',\n",
       " '1124',\n",
       " '1124 nod',\n",
       " '1124 nod means',\n",
       " '1124 return',\n",
       " '1124 return dr',\n",
       " '1125',\n",
       " '1125 hindoos',\n",
       " '1125 hindoos mr',\n",
       " '1125 return',\n",
       " '1125 return tylor',\n",
       " '1126',\n",
       " '1126 latter',\n",
       " '1126 latter movement',\n",
       " '1126 return',\n",
       " '1126 return lubbock',\n",
       " '1129',\n",
       " '1129 fishes',\n",
       " '1129 fishes abounded',\n",
       " '113',\n",
       " '113 163',\n",
       " '113 163 domesticated',\n",
       " '113 breslau',\n",
       " '113 breslau 114',\n",
       " '113 days',\n",
       " '113 days little',\n",
       " '113 males',\n",
       " '113 males forest',\n",
       " '113 observed',\n",
       " '113 observed however',\n",
       " '113 return',\n",
       " '113 return muller',\n",
       " '113 scraped',\n",
       " '113 scraped across',\n",
       " '113 species',\n",
       " '113 species price',\n",
       " '113 young',\n",
       " '113 young blackbird',\n",
       " '1134',\n",
       " '1134 another',\n",
       " '1134 another district',\n",
       " '114',\n",
       " '114 112',\n",
       " '114 112 mr',\n",
       " '114 122',\n",
       " '114 122 moreau',\n",
       " '114 1862',\n",
       " '114 1862 97',\n",
       " '114 806',\n",
       " '114 806 return',\n",
       " '114 chap',\n",
       " '114 chap special',\n",
       " '114 days',\n",
       " '114 days old',\n",
       " '114 furnished',\n",
       " '114 furnished small',\n",
       " '114 livingstone',\n",
       " '114 livingstone expedition',\n",
       " '114 livonia',\n",
       " '114 livonia 120',\n",
       " '114 often',\n",
       " '114 often causes',\n",
       " '114 rarer',\n",
       " '114 rarer species',\n",
       " '114 return',\n",
       " '114 return dr',\n",
       " '114 several',\n",
       " '114 several genera',\n",
       " '115',\n",
       " '115 116',\n",
       " '115 116 adduces',\n",
       " '115 145',\n",
       " '115 145 chap',\n",
       " '115 407',\n",
       " '115 407 return',\n",
       " '115 full',\n",
       " '115 full account',\n",
       " '115 look',\n",
       " '115 look intellectual',\n",
       " '115 return',\n",
       " '115 return see',\n",
       " '115 saw',\n",
       " '115 saw two',\n",
       " '115 table',\n",
       " '115 table intemperate',\n",
       " '115 take',\n",
       " '115 take much',\n",
       " '116',\n",
       " '116 95',\n",
       " '116 95 larger',\n",
       " '116 adduces',\n",
       " '116 adduces camper',\n",
       " '116 enters',\n",
       " '116 enters detail',\n",
       " '116 excellent',\n",
       " '116 excellent account',\n",
       " '116 males',\n",
       " '116 males 100',\n",
       " '116 return',\n",
       " '116 return dr',\n",
       " '116 several',\n",
       " '116 several regions',\n",
       " '116 sic',\n",
       " '116 sic saw',\n",
       " '117',\n",
       " '117 131',\n",
       " '117 131 138',\n",
       " '117 ample',\n",
       " '117 ample time',\n",
       " '117 elongation',\n",
       " '117 elongation skull',\n",
       " '117 following',\n",
       " '117 following statements',\n",
       " '117 recently',\n",
       " '117 recently stated',\n",
       " '117 return',\n",
       " '117 return carpenter',\n",
       " '118',\n",
       " '118 121',\n",
       " '118 121 915',\n",
       " '118 121 fancier',\n",
       " '118 1403',\n",
       " '118 1403 return',\n",
       " '118 additional',\n",
       " '118 additional evidence',\n",
       " '118 certain',\n",
       " '118 certain associated',\n",
       " '118 charadrius',\n",
       " '118 charadrius pluvialis',\n",
       " '118 give',\n",
       " '118 give one',\n",
       " '118 noticed',\n",
       " '118 noticed medical',\n",
       " '118 return',\n",
       " '118 return mowbray',\n",
       " '118 see',\n",
       " '118 see also',\n",
       " '118 wild',\n",
       " '118 wild blackbirds',\n",
       " '119',\n",
       " '119 effect',\n",
       " '119 effect lopping',\n",
       " '119 improved',\n",
       " '119 improved breeds',\n",
       " '119 likewise',\n",
       " '119 likewise describes',\n",
       " '119 proved',\n",
       " '119 proved innate',\n",
       " '119 return',\n",
       " '119 return see',\n",
       " '119 therefore',\n",
       " '119 therefore may',\n",
       " '119 tropical',\n",
       " '119 tropical sun',\n",
       " '119 widely',\n",
       " '119 widely distinct',\n",
       " '12',\n",
       " '12 000',\n",
       " '12 000 feet',\n",
       " '12 132',\n",
       " '12 132 scandinavia',\n",
       " '12 1805',\n",
       " '12 1805 another',\n",
       " '12 1836',\n",
       " '12 1836 canines',\n",
       " '12 1871',\n",
       " '12 1871 320',\n",
       " '12 35',\n",
       " '12 35 analogous',\n",
       " '12 73',\n",
       " '12 73 16',\n",
       " '12 763',\n",
       " '12 763 males',\n",
       " '12 797',\n",
       " '12 797 females',\n",
       " '12 873',\n",
       " '12 873 high',\n",
       " '12 acclimatization',\n",
       " '12 acclimatization parrots',\n",
       " '12 also',\n",
       " '12 also attributed',\n",
       " '12 audubon',\n",
       " '12 audubon ornith',\n",
       " '12 birds',\n",
       " '12 birds may',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature names of tokens are stored in TfidfVectorizer object \n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:51:45.489219Z",
     "start_time": "2018-01-19T22:51:45.479226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 552738)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T19:50:55.895613Z",
     "start_time": "2018-01-20T19:50:55.872596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to convert sparse document-term matrix 'X' to an numpy array to allow it to be put into pandas df\n",
    "# this can be done on-the-fly below when making the dataframe.  Just showing it explicitly here with type()\n",
    "\n",
    "X_arr = X.toarray()\n",
    "\n",
    "type(X_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:00:37.343758Z",
     "start_time": "2018-01-20T20:00:36.284391Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a pandas dataframe from the doc-term matrix containing the docs and features with the Tf-idf scores are the entries \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "darwin_corpus_tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:01:18.544848Z",
     "start_time": "2018-01-20T20:01:18.493814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>000 000</th>\n",
       "      <th>000 000 soldiers</th>\n",
       "      <th>000 15</th>\n",
       "      <th>000 15 000</th>\n",
       "      <th>000 according</th>\n",
       "      <th>000 according loose</th>\n",
       "      <th>000 feet</th>\n",
       "      <th>000 feet deeply</th>\n",
       "      <th>000 feet informs</th>\n",
       "      <th>...</th>\n",
       "      <th>über das darwin</th>\n",
       "      <th>über das langenwachsthum</th>\n",
       "      <th>über den</th>\n",
       "      <th>über den vogelschwanz</th>\n",
       "      <th>über die</th>\n",
       "      <th>über die darwin</th>\n",
       "      <th>über die entstehung</th>\n",
       "      <th>über die heuschrecken</th>\n",
       "      <th>über die knochernen</th>\n",
       "      <th>über die richtung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 552738 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000   000 000  000 000 soldiers    000 15  000 15 000  000 according  \\\n",
       "0  0.002388  0.000000          0.000000  0.000000    0.000000       0.000000   \n",
       "1  0.001552  0.000292          0.000292  0.000292    0.000292       0.000292   \n",
       "2  0.000995  0.000000          0.000000  0.000000    0.000000       0.000000   \n",
       "\n",
       "   000 according loose  000 feet  000 feet deeply  000 feet informs  \\\n",
       "0             0.000000  0.001537         0.000404          0.000000   \n",
       "1             0.000292  0.000222         0.000000          0.000292   \n",
       "2             0.000000  0.000000         0.000000          0.000000   \n",
       "\n",
       "         ...          über das darwin  über das langenwachsthum  über den  \\\n",
       "0        ...                 0.000000                  0.000000  0.000000   \n",
       "1        ...                 0.000292                  0.000292  0.000292   \n",
       "2        ...                 0.000000                  0.000000  0.000000   \n",
       "\n",
       "   über den vogelschwanz  über die  über die darwin  über die entstehung  \\\n",
       "0               0.000000  0.000000         0.000000             0.000000   \n",
       "1               0.000292  0.001752         0.000292             0.000292   \n",
       "2               0.000000  0.000000         0.000000             0.000000   \n",
       "\n",
       "   über die heuschrecken  über die knochernen  über die richtung  \n",
       "0               0.000000             0.000000           0.000000  \n",
       "1               0.000292             0.000292           0.000584  \n",
       "2               0.000000             0.000000           0.000000  \n",
       "\n",
       "[3 rows x 552738 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darwin_corpus_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:06:27.606378Z",
     "start_time": "2018-01-20T20:06:27.581360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 soldiers</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 15 000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 according</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 according loose</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 feet</th>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 feet deeply</th>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 feet informs</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 feet professor</th>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 feet solid</th>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 feet valleys</th>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 feet yet</th>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 number</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 number soon</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 padding</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 padding 5em</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 particularly</th>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 particularly important</th>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1         2\n",
       "000                         0.002388  0.001552  0.000995\n",
       "000 000                     0.000000  0.000292  0.000000\n",
       "000 000 soldiers            0.000000  0.000292  0.000000\n",
       "000 15                      0.000000  0.000292  0.000000\n",
       "000 15 000                  0.000000  0.000292  0.000000\n",
       "000 according               0.000000  0.000292  0.000000\n",
       "000 according loose         0.000000  0.000292  0.000000\n",
       "000 feet                    0.001537  0.000222  0.000000\n",
       "000 feet deeply             0.000404  0.000000  0.000000\n",
       "000 feet informs            0.000000  0.000292  0.000000\n",
       "000 feet professor          0.000404  0.000000  0.000000\n",
       "000 feet solid              0.000404  0.000000  0.000000\n",
       "000 feet valleys            0.000404  0.000000  0.000000\n",
       "000 feet yet                0.000404  0.000000  0.000000\n",
       "000 number                  0.000000  0.000292  0.000000\n",
       "000 number soon             0.000000  0.000292  0.000000\n",
       "000 padding                 0.000307  0.000000  0.000641\n",
       "000 padding 5em             0.000307  0.000000  0.000641\n",
       "000 particularly            0.000239  0.000172  0.000498\n",
       "000 particularly important  0.000239  0.000172  0.000498"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transposed dataframe is now the term-document matrix cast as a df\n",
    "\n",
    "darwin_corpus_tfidf_df.transpose().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:07:26.120107Z",
     "start_time": "2018-01-20T20:07:26.102095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>étalons qui</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>étalons qui eprennent</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>étant</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>étant plus</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>étant plus terne</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>études</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>études sur</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>études sur les</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über das</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über das darwin</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über das langenwachsthum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über den</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über den vogelschwanz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über die</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über die darwin</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über die entstehung</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über die heuschrecken</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über die knochernen</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über die richtung</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0         1    2\n",
       "étalons qui               0.0  0.000292  0.0\n",
       "étalons qui eprennent     0.0  0.000292  0.0\n",
       "étant                     0.0  0.000292  0.0\n",
       "étant plus                0.0  0.000292  0.0\n",
       "étant plus terne          0.0  0.000292  0.0\n",
       "études                    0.0  0.000876  0.0\n",
       "études sur                0.0  0.000876  0.0\n",
       "études sur les            0.0  0.000876  0.0\n",
       "über                      0.0  0.002628  0.0\n",
       "über das                  0.0  0.000584  0.0\n",
       "über das darwin           0.0  0.000292  0.0\n",
       "über das langenwachsthum  0.0  0.000292  0.0\n",
       "über den                  0.0  0.000292  0.0\n",
       "über den vogelschwanz     0.0  0.000292  0.0\n",
       "über die                  0.0  0.001752  0.0\n",
       "über die darwin           0.0  0.000292  0.0\n",
       "über die entstehung       0.0  0.000292  0.0\n",
       "über die heuschrecken     0.0  0.000292  0.0\n",
       "über die knochernen       0.0  0.000292  0.0\n",
       "über die richtung         0.0  0.000584  0.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darwin_corpus_tfidf_df.transpose().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:12:13.570324Z",
     "start_time": "2018-01-20T20:12:13.432228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species              0.460826\n",
       "one                  0.193642\n",
       "would                0.167855\n",
       "may                  0.161886\n",
       "many                 0.147321\n",
       "forms                0.134905\n",
       "selection            0.133950\n",
       "natural              0.127742\n",
       "varieties            0.116042\n",
       "two                  0.112699\n",
       "plants               0.112461\n",
       "animals              0.104104\n",
       "natural selection    0.097418\n",
       "thus                 0.093359\n",
       "several              0.087867\n",
       "different            0.087390\n",
       "great                0.086435\n",
       "distinct             0.085718\n",
       "life                 0.083808\n",
       "case                 0.081898\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 tf-idf scores for 'On the Origin of Species' relative to the three book darwin_corpus\n",
    "\n",
    "darwin_corpus_tfidf_df.transpose()[0].sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:14:07.094148Z",
     "start_time": "2018-01-20T20:14:06.966055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male         0.274739\n",
       "males        0.219722\n",
       "man          0.210927\n",
       "female       0.197992\n",
       "species      0.189713\n",
       "sexes        0.180400\n",
       "females      0.170225\n",
       "one          0.163843\n",
       "birds        0.160739\n",
       "mr           0.155392\n",
       "would        0.150046\n",
       "many         0.130902\n",
       "sexual       0.126590\n",
       "may          0.122624\n",
       "animals      0.115380\n",
       "selection    0.107102\n",
       "young        0.096926\n",
       "two          0.096754\n",
       "vol          0.094339\n",
       "much         0.093994\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 tf-idf scores for 'The Descent of Man' relative to the three book darwin_corpus\n",
    "\n",
    "darwin_corpus_tfidf_df.transpose()[1].sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:14:52.835574Z",
     "start_time": "2018-01-20T20:14:52.730496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "may           0.173657\n",
       "one           0.173160\n",
       "man           0.169179\n",
       "muscles       0.167686\n",
       "return        0.160223\n",
       "expression    0.154251\n",
       "eyes          0.134846\n",
       "mr            0.128377\n",
       "much          0.120913\n",
       "often         0.117430\n",
       "movements     0.112454\n",
       "tears         0.106996\n",
       "mouth         0.103995\n",
       "blush         0.096043\n",
       "eyebrows      0.095469\n",
       "action        0.094541\n",
       "would         0.094044\n",
       "seen          0.094044\n",
       "thus          0.092053\n",
       "manner        0.090561\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 tf-idf scores for 'The Expression of the Emotions in Man and Animal' relative to the three book darwin_corpus\n",
    "\n",
    "darwin_corpus_tfidf_df.transpose()[2].sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:56:57.686297Z",
     "start_time": "2018-01-19T22:56:51.683026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=3, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a matrix decomposition to do latent semantic analysis (create three concepts-- single \"concept\" for each book)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa = TruncatedSVD(n_components=3, n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T22:59:57.567703Z",
     "start_time": "2018-01-19T22:59:57.556694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00200425,  0.00012308,  0.00012308, ...,  0.00012308,\n",
       "        0.00012308,  0.00024616])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concept (rows) by terms (three columns) matrix that is part of decomposition can be\n",
    "# accessed by lsa.components_[]\n",
    "\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:00:29.386875Z",
     "start_time": "2018-01-19T23:00:29.380867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 552738)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:06:56.650858Z",
     "start_time": "2018-01-19T23:06:54.574916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "species\n",
      "one\n",
      "may\n",
      "would\n",
      "man\n",
      "many\n",
      "mr\n",
      "male\n",
      "animals\n",
      "much\n",
      "thus\n",
      "two\n",
      "see\n",
      "selection\n",
      "males\n",
      "often\n",
      "birds\n",
      "case\n",
      "female\n",
      "certain\n",
      " \n",
      "Concept 1:\n",
      "muscles\n",
      "return\n",
      "expression\n",
      "eyes\n",
      "movements\n",
      "tears\n",
      "mouth\n",
      "blush\n",
      "eyebrows\n",
      "man\n",
      "face\n",
      "mind\n",
      "action\n",
      "habit\n",
      "seen\n",
      "body\n",
      "laughter\n",
      "contraction\n",
      "often\n",
      "blushing\n",
      " \n",
      "Concept 2:\n",
      "male\n",
      "males\n",
      "female\n",
      "sexes\n",
      "females\n",
      "man\n",
      "birds\n",
      "sexual\n",
      "vol\n",
      "colour\n",
      "mr\n",
      "colours\n",
      "coloured\n",
      "young\n",
      "plumage\n",
      "sexual selection\n",
      "men\n",
      "horns\n",
      "sex\n",
      "shewn\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# print top 20 terms for each concept\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedTerms= sorted(termsInComp, key=lambda x: x[1], reverse=True)[:20]\n",
    "    print('Concept %d:' % i)\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following Data School (Kevin Markham) tutorial on using scikit learn with text\n",
    "\n",
    "# https://www.youtube.com/watch?v=8QmkFAthuPU\n",
    "\n",
    "# Basic workflow of scikit learn ML model building:\n",
    "\n",
    "# i.) Import Python libraries \n",
    "# ii.) Instantiate model\n",
    "# iii.) Fit model to training data\n",
    "# iv.)  Predict on test data and other out-of-sample data (or transform in case of working with text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:04:03.292914Z",
     "start_time": "2018-01-20T16:04:03.284914Z"
    }
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with default parameters)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:07:53.841870Z",
     "start_time": "2018-01-20T16:07:53.059868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting CountVectorizer() to darwin_corpus (no preprocessing on text besides just extracting text from html).  \n",
    "# Note that CountVectorizer() expects a list or iterable argument\n",
    "# Fitting learns the 'vocabulary' of the training data (occurs in-place)\n",
    "\n",
    "# NOTICE THERE ARE NO STOPWORDS REMOVED AS DEFAULT SETTING\n",
    "\n",
    "# ALSO, fitting happens in place (no need to store output with variable)\n",
    "\n",
    "\n",
    "vect.fit(darwin_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:08:59.914590Z",
     "start_time": "2018-01-20T16:08:59.864571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '019',\n",
       " '050',\n",
       " '071',\n",
       " '08',\n",
       " '084',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1001',\n",
       " '1002',\n",
       " '1003',\n",
       " '1004',\n",
       " '1005',\n",
       " '1006',\n",
       " '1007',\n",
       " '1008',\n",
       " '1009',\n",
       " '101',\n",
       " '1010',\n",
       " '1011',\n",
       " '1012',\n",
       " '1013',\n",
       " '1014',\n",
       " '1015',\n",
       " '1016',\n",
       " '1017',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '1040',\n",
       " '105',\n",
       " '106',\n",
       " '1068',\n",
       " '107',\n",
       " '108',\n",
       " '1085',\n",
       " '109',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '1101',\n",
       " '1102',\n",
       " '1103',\n",
       " '1104',\n",
       " '1105',\n",
       " '1106',\n",
       " '1107',\n",
       " '1108',\n",
       " '1109',\n",
       " '111',\n",
       " '1110',\n",
       " '1111',\n",
       " '1112',\n",
       " '1113',\n",
       " '1114',\n",
       " '1115',\n",
       " '1116',\n",
       " '1117',\n",
       " '1118',\n",
       " '1119',\n",
       " '112',\n",
       " '1120',\n",
       " '1121',\n",
       " '1122',\n",
       " '1123',\n",
       " '1124',\n",
       " '1125',\n",
       " '1126',\n",
       " '1129',\n",
       " '113',\n",
       " '1134',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '1201',\n",
       " '1202',\n",
       " '1203',\n",
       " '1204',\n",
       " '1205',\n",
       " '1206',\n",
       " '1207',\n",
       " '1208',\n",
       " '1209',\n",
       " '121',\n",
       " '1210',\n",
       " '1211',\n",
       " '1212',\n",
       " '1213',\n",
       " '1214',\n",
       " '1215',\n",
       " '1216',\n",
       " '1217',\n",
       " '1218',\n",
       " '1219',\n",
       " '122',\n",
       " '1220',\n",
       " '1221',\n",
       " '1222',\n",
       " '1223',\n",
       " '1224',\n",
       " '1225',\n",
       " '1226',\n",
       " '1227',\n",
       " '1228',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '125th',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '12s',\n",
       " '13',\n",
       " '130',\n",
       " '1301',\n",
       " '1302',\n",
       " '1303',\n",
       " '1304',\n",
       " '1305',\n",
       " '1306',\n",
       " '1307',\n",
       " '1308',\n",
       " '1309',\n",
       " '131',\n",
       " '1310',\n",
       " '1311',\n",
       " '1312',\n",
       " '1313',\n",
       " '1314',\n",
       " '1315',\n",
       " '1316',\n",
       " '1317',\n",
       " '1318',\n",
       " '1319',\n",
       " '132',\n",
       " '1320',\n",
       " '1321',\n",
       " '1322',\n",
       " '1323',\n",
       " '1324',\n",
       " '1325',\n",
       " '1326',\n",
       " '1327',\n",
       " '1328',\n",
       " '1329',\n",
       " '133',\n",
       " '1330',\n",
       " '1331',\n",
       " '1332',\n",
       " '1333',\n",
       " '1334',\n",
       " '134',\n",
       " '1340',\n",
       " '1341',\n",
       " '1342',\n",
       " '1343',\n",
       " '1344',\n",
       " '1345',\n",
       " '1346',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '14',\n",
       " '140',\n",
       " '1401',\n",
       " '1402',\n",
       " '1403',\n",
       " '1404',\n",
       " '1405',\n",
       " '141',\n",
       " '1415',\n",
       " '142',\n",
       " '1426',\n",
       " '1429',\n",
       " '143',\n",
       " '1430',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '1484',\n",
       " '149',\n",
       " '1496',\n",
       " '1499',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1503',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '1548',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '15s',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1602',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '1667',\n",
       " '167',\n",
       " '168',\n",
       " '169',\n",
       " '1699',\n",
       " '17',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '174',\n",
       " '1746',\n",
       " '1747',\n",
       " '175',\n",
       " '1751',\n",
       " '1753',\n",
       " '176',\n",
       " '1767',\n",
       " '177',\n",
       " '1773',\n",
       " '1774',\n",
       " '1775',\n",
       " '1777',\n",
       " '1778',\n",
       " '1779',\n",
       " '178',\n",
       " '1780',\n",
       " '1782',\n",
       " '1788',\n",
       " '179',\n",
       " '1791',\n",
       " '1792',\n",
       " '1793',\n",
       " '1794',\n",
       " '1795',\n",
       " '1796',\n",
       " '1797',\n",
       " '1799',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1801',\n",
       " '1802',\n",
       " '1805',\n",
       " '1806',\n",
       " '1807',\n",
       " '1808',\n",
       " '1809',\n",
       " '181',\n",
       " '1810',\n",
       " '1812',\n",
       " '1813',\n",
       " '1815',\n",
       " '1816',\n",
       " '1818',\n",
       " '1819',\n",
       " '182',\n",
       " '1820',\n",
       " '1821',\n",
       " '1822',\n",
       " '1823',\n",
       " '1824',\n",
       " '1825',\n",
       " '1826',\n",
       " '1828',\n",
       " '1829',\n",
       " '183',\n",
       " '1830',\n",
       " '1831',\n",
       " '1832',\n",
       " '1833',\n",
       " '1834',\n",
       " '1835',\n",
       " '1836',\n",
       " '1837',\n",
       " '1838',\n",
       " '1839',\n",
       " '184',\n",
       " '1840',\n",
       " '1841',\n",
       " '1842',\n",
       " '1843',\n",
       " '1844',\n",
       " '1845',\n",
       " '1846',\n",
       " '1847',\n",
       " '1848',\n",
       " '1849',\n",
       " '185',\n",
       " '1850',\n",
       " '1851',\n",
       " '1852',\n",
       " '1853',\n",
       " '1854',\n",
       " '1855',\n",
       " '1856',\n",
       " '1857',\n",
       " '1858',\n",
       " '1859',\n",
       " '186',\n",
       " '1860',\n",
       " '1861',\n",
       " '1862',\n",
       " '1863',\n",
       " '1864',\n",
       " '1865',\n",
       " '1866',\n",
       " '1867',\n",
       " '1868',\n",
       " '1869',\n",
       " '187',\n",
       " '1870',\n",
       " '1871',\n",
       " '1872',\n",
       " '1873',\n",
       " '1874',\n",
       " '1876',\n",
       " '188',\n",
       " '1887',\n",
       " '189',\n",
       " '1896',\n",
       " '1899',\n",
       " '19',\n",
       " '190',\n",
       " '191',\n",
       " '192',\n",
       " '193',\n",
       " '194',\n",
       " '195',\n",
       " '196',\n",
       " '197',\n",
       " '198',\n",
       " '199',\n",
       " '1998',\n",
       " '1999',\n",
       " '1em',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2009',\n",
       " '201',\n",
       " '2012',\n",
       " '2013',\n",
       " '2017',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '2048',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '20th',\n",
       " '21',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '2129',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '21s',\n",
       " '22',\n",
       " '220',\n",
       " '221',\n",
       " '222',\n",
       " '223',\n",
       " '224',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '22nd',\n",
       " '23',\n",
       " '230',\n",
       " '2300',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '236',\n",
       " '237',\n",
       " '238',\n",
       " '239',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '253',\n",
       " '254',\n",
       " '255',\n",
       " '256',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '25em',\n",
       " '26',\n",
       " '260',\n",
       " '2600',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '264',\n",
       " '265',\n",
       " '266',\n",
       " '268',\n",
       " '269',\n",
       " '27',\n",
       " '271',\n",
       " '272',\n",
       " '274',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '279',\n",
       " '28',\n",
       " '280',\n",
       " '281',\n",
       " '282',\n",
       " '283',\n",
       " '284',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '289',\n",
       " '29',\n",
       " '290',\n",
       " '292',\n",
       " '293',\n",
       " '294',\n",
       " '295',\n",
       " '296',\n",
       " '297',\n",
       " '298',\n",
       " '299',\n",
       " '2nd',\n",
       " '2s',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '301',\n",
       " '302',\n",
       " '303',\n",
       " '304',\n",
       " '305',\n",
       " '306',\n",
       " '307',\n",
       " '308',\n",
       " '309',\n",
       " '31',\n",
       " '310',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '314',\n",
       " '315',\n",
       " '316',\n",
       " '317',\n",
       " '318',\n",
       " '319',\n",
       " '32',\n",
       " '320',\n",
       " '321',\n",
       " '322',\n",
       " '323',\n",
       " '324',\n",
       " '325',\n",
       " '326',\n",
       " '327',\n",
       " '3273',\n",
       " '328',\n",
       " '329',\n",
       " '33',\n",
       " '330',\n",
       " '331',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '34',\n",
       " '340',\n",
       " '341',\n",
       " '342',\n",
       " '343',\n",
       " '344',\n",
       " '345',\n",
       " '346',\n",
       " '347',\n",
       " '348',\n",
       " '349',\n",
       " '35',\n",
       " '350',\n",
       " '351',\n",
       " '352',\n",
       " '353',\n",
       " '354',\n",
       " '355',\n",
       " '356',\n",
       " '357',\n",
       " '358',\n",
       " '359',\n",
       " '36',\n",
       " '360',\n",
       " '3605',\n",
       " '361',\n",
       " '362',\n",
       " '363',\n",
       " '364',\n",
       " '365',\n",
       " '366',\n",
       " '369',\n",
       " '36s',\n",
       " '37',\n",
       " '370',\n",
       " '371',\n",
       " '372',\n",
       " '373',\n",
       " '375',\n",
       " '376',\n",
       " '377',\n",
       " '3776',\n",
       " '378',\n",
       " '379',\n",
       " '38',\n",
       " '381',\n",
       " '382',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '386',\n",
       " '387',\n",
       " '388',\n",
       " '389',\n",
       " '39',\n",
       " '390',\n",
       " '391',\n",
       " '392',\n",
       " '393',\n",
       " '394',\n",
       " '3946',\n",
       " '395',\n",
       " '396',\n",
       " '397',\n",
       " '398',\n",
       " '3em',\n",
       " '3rd',\n",
       " '3s',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '401',\n",
       " '402',\n",
       " '403',\n",
       " '404',\n",
       " '405',\n",
       " '406',\n",
       " '407',\n",
       " '408',\n",
       " '409',\n",
       " '41',\n",
       " '410',\n",
       " '411',\n",
       " '412',\n",
       " '413',\n",
       " '414',\n",
       " '41421',\n",
       " '415',\n",
       " '416',\n",
       " '417',\n",
       " '418',\n",
       " '419',\n",
       " '42',\n",
       " '420',\n",
       " '421',\n",
       " '422',\n",
       " '423',\n",
       " '424',\n",
       " '425',\n",
       " '426',\n",
       " '427',\n",
       " '428',\n",
       " '429',\n",
       " '42b',\n",
       " '43',\n",
       " '430',\n",
       " '431',\n",
       " '432',\n",
       " '433',\n",
       " '434',\n",
       " '435',\n",
       " '436',\n",
       " '437',\n",
       " '439',\n",
       " '44',\n",
       " '440',\n",
       " '4407',\n",
       " '441',\n",
       " '443',\n",
       " '444',\n",
       " '445',\n",
       " '446',\n",
       " '447',\n",
       " '448',\n",
       " '449',\n",
       " '45',\n",
       " '450',\n",
       " '451',\n",
       " '452',\n",
       " '453',\n",
       " '455',\n",
       " '4557',\n",
       " '4558',\n",
       " '456',\n",
       " '457',\n",
       " '458',\n",
       " '459',\n",
       " '46',\n",
       " '460',\n",
       " '461',\n",
       " '462',\n",
       " '463',\n",
       " '466',\n",
       " '468',\n",
       " '469',\n",
       " '47',\n",
       " '471',\n",
       " '472',\n",
       " '4723',\n",
       " '473',\n",
       " '475',\n",
       " '476',\n",
       " '477',\n",
       " '478',\n",
       " '479',\n",
       " '48',\n",
       " '481',\n",
       " '482',\n",
       " '483',\n",
       " '484',\n",
       " '485',\n",
       " '486',\n",
       " '487',\n",
       " '488',\n",
       " '489',\n",
       " '49',\n",
       " '492',\n",
       " '494',\n",
       " '495',\n",
       " '496',\n",
       " '497',\n",
       " '498',\n",
       " '4th',\n",
       " '4to',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '501',\n",
       " '502',\n",
       " '503',\n",
       " '504',\n",
       " '505',\n",
       " '506',\n",
       " '507',\n",
       " '508',\n",
       " '509',\n",
       " '51',\n",
       " '510',\n",
       " '511',\n",
       " '512',\n",
       " '513',\n",
       " '514',\n",
       " '515',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '52',\n",
       " '520',\n",
       " '521',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '525',\n",
       " '526',\n",
       " '527',\n",
       " '528',\n",
       " '529',\n",
       " '53',\n",
       " '530',\n",
       " '531',\n",
       " '532',\n",
       " '533',\n",
       " '534',\n",
       " '535',\n",
       " '537',\n",
       " '539',\n",
       " '54',\n",
       " '541',\n",
       " '542',\n",
       " '543',\n",
       " '544',\n",
       " '545',\n",
       " '546',\n",
       " '55',\n",
       " '550',\n",
       " '551',\n",
       " '552',\n",
       " '553',\n",
       " '555',\n",
       " '557',\n",
       " '558',\n",
       " '56',\n",
       " '560',\n",
       " '561',\n",
       " '562',\n",
       " '563',\n",
       " '564',\n",
       " '565',\n",
       " '567',\n",
       " '568',\n",
       " '569',\n",
       " '57',\n",
       " '570',\n",
       " '571',\n",
       " '573',\n",
       " '574',\n",
       " '576',\n",
       " '579',\n",
       " '58',\n",
       " '581',\n",
       " '583',\n",
       " '584',\n",
       " '585',\n",
       " '586',\n",
       " '589',\n",
       " '59',\n",
       " '595',\n",
       " '596',\n",
       " '598',\n",
       " '5em',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '601',\n",
       " '602',\n",
       " '603',\n",
       " '604',\n",
       " '605',\n",
       " '606',\n",
       " '607',\n",
       " '608',\n",
       " '609',\n",
       " '61',\n",
       " '610',\n",
       " '611',\n",
       " '612',\n",
       " '613',\n",
       " '614',\n",
       " '615',\n",
       " '616',\n",
       " '617',\n",
       " '618',\n",
       " '619',\n",
       " '62',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '6221541',\n",
       " '623',\n",
       " '624',\n",
       " '625',\n",
       " '626',\n",
       " '627',\n",
       " '63',\n",
       " '630',\n",
       " '632',\n",
       " '633',\n",
       " '634',\n",
       " '637',\n",
       " '638',\n",
       " '639',\n",
       " '64',\n",
       " '641',\n",
       " '642',\n",
       " '647',\n",
       " '65',\n",
       " '650',\n",
       " '651',\n",
       " '657',\n",
       " '659',\n",
       " '66',\n",
       " '660',\n",
       " '667',\n",
       " '67',\n",
       " '671',\n",
       " '672',\n",
       " '675',\n",
       " '676',\n",
       " '677',\n",
       " '68',\n",
       " '681',\n",
       " '682',\n",
       " '683',\n",
       " '685',\n",
       " '687',\n",
       " '6878',\n",
       " '69',\n",
       " '690',\n",
       " '6d',\n",
       " '6s',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '7000',\n",
       " '701',\n",
       " '702',\n",
       " '703',\n",
       " '704',\n",
       " '705',\n",
       " '706',\n",
       " '707',\n",
       " '709',\n",
       " '71',\n",
       " '711',\n",
       " '715',\n",
       " '72',\n",
       " '721',\n",
       " '722',\n",
       " '724',\n",
       " '725',\n",
       " '729',\n",
       " '73',\n",
       " '730',\n",
       " '731',\n",
       " '732',\n",
       " '738',\n",
       " '7385',\n",
       " '739',\n",
       " '74',\n",
       " '740',\n",
       " '743',\n",
       " '745',\n",
       " '746',\n",
       " '747',\n",
       " '749',\n",
       " '75',\n",
       " '750',\n",
       " '751',\n",
       " '753',\n",
       " '75em',\n",
       " '76',\n",
       " '761',\n",
       " '762',\n",
       " '763',\n",
       " '764',\n",
       " '765',\n",
       " '77',\n",
       " '771',\n",
       " '773',\n",
       " '774',\n",
       " '775',\n",
       " '778',\n",
       " '78',\n",
       " '780',\n",
       " '786',\n",
       " '79',\n",
       " '794',\n",
       " '796',\n",
       " '797',\n",
       " '798',\n",
       " '7s',\n",
       " '7th',\n",
       " '80',\n",
       " '801',\n",
       " '802',\n",
       " '803',\n",
       " '804',\n",
       " '805',\n",
       " '806',\n",
       " '807',\n",
       " '808',\n",
       " '8081',\n",
       " '809',\n",
       " '81',\n",
       " '810',\n",
       " '811',\n",
       " '812',\n",
       " '813',\n",
       " '814',\n",
       " '815',\n",
       " '816',\n",
       " '817',\n",
       " '818',\n",
       " '819',\n",
       " '82',\n",
       " '820',\n",
       " '821',\n",
       " '822',\n",
       " '823',\n",
       " '824',\n",
       " '825',\n",
       " '826',\n",
       " '827',\n",
       " '83',\n",
       " '836',\n",
       " '84',\n",
       " '84116',\n",
       " '847',\n",
       " '85',\n",
       " '86',\n",
       " '865',\n",
       " '866',\n",
       " '868',\n",
       " '87',\n",
       " '873',\n",
       " '876',\n",
       " '88',\n",
       " '8859',\n",
       " '89',\n",
       " '890',\n",
       " '8965',\n",
       " '8th',\n",
       " '8ths',\n",
       " '8vo',\n",
       " '90',\n",
       " '900',\n",
       " '901',\n",
       " '902',\n",
       " '903',\n",
       " '904',\n",
       " '905',\n",
       " '906',\n",
       " '907',\n",
       " '908',\n",
       " '909',\n",
       " '91',\n",
       " '910',\n",
       " '911',\n",
       " '912',\n",
       " '913',\n",
       " '914',\n",
       " '915',\n",
       " '916',\n",
       " '92',\n",
       " '924',\n",
       " '93',\n",
       " '932',\n",
       " '934',\n",
       " '936',\n",
       " '937',\n",
       " '938',\n",
       " '939',\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "# returns vocabulary in numerical and then alphabetical order\n",
    "\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:10:00.866009Z",
     "start_time": "2018-01-20T16:10:00.299565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x19828 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 32176 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "# this will be a sparse array where documents (each of the three books) are rows and terms (i.e. vocabulary found) are columns\n",
    "\n",
    "darwin_corpus_dtm = vect.transform(darwin_corpus)\n",
    "darwin_corpus_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:18:26.335648Z",
     "start_time": "2018-01-20T16:18:26.324637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0, ...,  0,  0,  0],\n",
       "       [ 9,  1,  1, ...,  1,  3,  9],\n",
       "       [ 2,  0,  0, ...,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix with toarray() method\n",
    "\n",
    "darwin_corpus_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T16:25:02.514099Z",
     "start_time": "2018-01-20T16:25:00.933055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>019</th>\n",
       "      <th>050</th>\n",
       "      <th>071</th>\n",
       "      <th>08</th>\n",
       "      <th>084</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>égard</th>\n",
       "      <th>élégance</th>\n",
       "      <th>émouvoir</th>\n",
       "      <th>époque</th>\n",
       "      <th>éprouve</th>\n",
       "      <th>étalage</th>\n",
       "      <th>étalons</th>\n",
       "      <th>étant</th>\n",
       "      <th>études</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 19828 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  019  050  071  08  084  09  10  100  1000  ...   égard  élégance  \\\n",
       "0   10    0    0    0   0    0   0   7    8     0  ...       0         0   \n",
       "1    9    1    1    1   1    1   1  34   57     3  ...       1         1   \n",
       "2    2    0    0    0   0    0   0  12    1     0  ...       0         0   \n",
       "\n",
       "   émouvoir  époque  éprouve  étalage  étalons  étant  études  über  \n",
       "0         0       0        0        0        0      0       0     0  \n",
       "1         1       2        1        1        1      1       3     9  \n",
       "2         0       0        0        0        0      0       0     0  \n",
       "\n",
       "[3 rows x 19828 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together by putting documents and terms (with unnormalized frequency counts) \n",
    "# into pandas dataframe \n",
    "# each document is now a \"bag-of-words\" (bag-of-words is result of basic tokenization, counting and often normalized frequency)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(darwin_corpus_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
